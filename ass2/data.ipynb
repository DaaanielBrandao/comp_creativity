{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Author</th>\n",
       "      <th>Title</th>\n",
       "      <th>Poetry Foundation ID</th>\n",
       "      <th>Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Wendy Videlock</td>\n",
       "      <td>!</td>\n",
       "      <td>55489</td>\n",
       "      <td>Dear Writers, I’m compiling the first in what ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>Kate Greenstreet</td>\n",
       "      <td>7 December</td>\n",
       "      <td>54602</td>\n",
       "      <td>Men are trading their bullets for worms.\\n“I s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>62</td>\n",
       "      <td>Carl Sandburg</td>\n",
       "      <td>The Abracadabra Boys\\n \\n \\n  \\n   Launch Audi...</td>\n",
       "      <td>53235</td>\n",
       "      <td>The abracadabra boys—have they been in the sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>63</td>\n",
       "      <td>Abraham Lincoln</td>\n",
       "      <td>Abraham Lincoln</td>\n",
       "      <td>45900</td>\n",
       "      <td>Abraham Lincoln\\nhis hand and pen\\nhe will be ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>79</td>\n",
       "      <td>Gillian Allnutt</td>\n",
       "      <td>abutment</td>\n",
       "      <td>118571</td>\n",
       "      <td>but for the askance in her\\nbut for the biding...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0            Author  \\\n",
       "0            0    Wendy Videlock   \n",
       "24          24  Kate Greenstreet   \n",
       "62          62     Carl Sandburg   \n",
       "63          63   Abraham Lincoln   \n",
       "79          79   Gillian Allnutt   \n",
       "\n",
       "                                                Title  Poetry Foundation ID  \\\n",
       "0                                                   !                 55489   \n",
       "24                                         7 December                 54602   \n",
       "62  The Abracadabra Boys\\n \\n \\n  \\n   Launch Audi...                 53235   \n",
       "63                                    Abraham Lincoln                 45900   \n",
       "79                                           abutment                118571   \n",
       "\n",
       "                                              Content  \n",
       "0   Dear Writers, I’m compiling the first in what ...  \n",
       "24  Men are trading their bullets for worms.\\n“I s...  \n",
       "62  The abracadabra boys—have they been in the sta...  \n",
       "63  Abraham Lincoln\\nhis hand and pen\\nhe will be ...  \n",
       "79  but for the askance in her\\nbut for the biding...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('kaggle_poem_dataset.csv')\n",
    "df = df[(df['Content'].str.count('\\n') < 8)]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import markovify\n",
    "corpus = df['Content'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "corpus = \"\\n\".join([line for line in random.sample(corpus, 1000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = markovify.NewlineText(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "In my father’s grave,\n",
      "For my daughter . . to show\n",
      "None\n",
      "\n",
      "Say I had dinner with a match\n",
      "\n",
      "trying to joke . . . Yes . . .\n",
      "There was a bamboo ladder.\n",
      "\n",
      "Try as much like the computer\n",
      "\n",
      "None\n",
      "None\n",
      "Each sack had seven kits:\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print()\n",
    "    for i in range(random.randrange(1, 4)):\n",
    "        print(model.make_short_sentence(30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM, Dense, Dropout, Flatten\n",
    "from keras.callbacks import LambdaCallback\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 102\n"
     ]
    }
   ],
   "source": [
    "# Lowercase all text\n",
    "text = corpus.lower()\n",
    "\n",
    "chars = list(set(text))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "vocab_size = len(chars)\n",
    "print('Vocabulary size: {}'.format(vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences: 444742\n"
     ]
    }
   ],
   "source": [
    "# Data preparation\n",
    "X = [] # training array\n",
    "Y = [] # target array\n",
    "\n",
    "length = len(text)\n",
    "seq_length = 100 # number of characters to consider before predicting a character\n",
    "\n",
    "# Iterate over length of text and create sequences stored in X, true values stored in Y\n",
    "# true values being which character would actually come after sequence stored in X\n",
    "for i in range(0, length - seq_length, 1):\n",
    "    sequence = text[i:i + seq_length]\n",
    "    label = text[i + seq_length]\n",
    "    X.append([char_indices[char] for char in sequence])\n",
    "    Y.append(char_indices[label])\n",
    "\n",
    "print('Number of sequences: {}'.format(len(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((444742, 100, 1), (444742, 102))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Reshape dimensions\n",
    "X_new = np.reshape(X, (len(X), seq_length, 1))\n",
    "# Scale values\n",
    "X_new = X_new/float(len(chars))\n",
    "# One-hot encode Y to remove ordinal relationships\n",
    "Y_new = np_utils.to_categorical(Y)\n",
    "\n",
    "X_new.shape, Y_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11131/13899 [=======================>......] - ETA: 5:02 - loss: 2.9846"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "# Add LSTM layer to compute output using 150 LSTM units\n",
    "model.add(LSTM(150, input_shape = (X_new.shape[1], X_new.shape[2]), return_sequences = True))\n",
    "\n",
    "# Add regularization layer to prevent overfitting.\n",
    "# Dropout ignores randomly selected neurons during training (\"dropped out\").\n",
    "# Ultimately, network becomes less sensitive to specific weights of neurons --> network is better at generalization.\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Flatten())\n",
    "# Dense layer with softmax activation function to approximate probability distribution of next best word\n",
    "model.add(Dense(Y_new.shape[1], activation = 'softmax'))\n",
    "\n",
    "# Compile model to configure learning process\n",
    "# Categorical crossentropy: an example can only belong to one class\n",
    "# Adam optimization algorithm updates a learning rate for each network weight iteratively as learning unfolds\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam')\n",
    "\n",
    "# Use 1 epoch for sake of computational time\n",
    "model.fit(X_new, Y_new, epochs = 1, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random start\n",
    "start = np.random.randint(0, len(X)-1)\n",
    "string_mapped = list(X[start])\n",
    "full_string = [indices_char[value] for value in string_mapped]\n",
    "\n",
    "# Generate text\n",
    "for i in range(400):\n",
    "    x = np.reshape(string_mapped, (1, len(string_mapped), 1))\n",
    "    x = x / float(len(chars))\n",
    "    \n",
    "    pred_index = np.argmax(model.predict(x, verbose = 0))\n",
    "    seq = [indices_char[value] for value in string_mapped]\n",
    "    full_string.append(indices_char[pred_index])\n",
    "    \n",
    "    string_mapped.append(pred_index)\n",
    "    string_mapped = string_mapped[1:len(string_mapped)]\n",
    "    \n",
    "# Combine text\n",
    "newtext = ''\n",
    "for char in full_string:\n",
    "    newtext = newtext + char\n",
    "\n",
    "print(newtext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check this https://www.kaggle.com/paultimothymooney/poetry-generator-rnn-markov because uses rhymes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict next words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.utils as ku\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding\n",
    "\n",
    "# Lowercase all text\n",
    "text = corpus.lower()\n",
    "text = text.split('\\n')\n",
    "\n",
    "# Create Tokenizer object to convert words to sequences of integers\n",
    "tokenizer = Tokenizer(num_words = None, filters = '#$%&()*+-<=>@[\\\\]^_`{|}~\\t\\n', lower = False)\n",
    "\n",
    "# Train tokenizer to the texts\n",
    "tokenizer.fit_on_texts(text)\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Convert list of strings into flat dataset of sequences of tokens\n",
    "sequences = []\n",
    "for line in text:\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        sequences.append(n_gram_sequence)\n",
    "\n",
    "# Pad sequences to ensure equal lengths\n",
    "max_seq_len = max([len(x) for x in sequences])\n",
    "sequences = np.array(pad_sequences(sequences, maxlen = max_seq_len, padding = 'pre'))\n",
    "\n",
    "# Create n-grams sequence predictors and labels\n",
    "predictors, label = sequences[:, :-1], sequences[:, -1]\n",
    "label = ku.to_categorical(label, num_classes = total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input layer takes sequence of words as input\n",
    "input_len = max_seq_len - 1\n",
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 10, input_length = input_len))\n",
    "model.add(LSTM(150))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(total_words, activation = 'softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam')\n",
    "\n",
    "# Use 100 epoch for efficacy\n",
    "model.fit(predictors, label, epochs = 100, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate line\n",
    "def generate_line(text, next_words, max_seq_len, model):\n",
    "    for j in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen = max_seq_len - 1, padding = 'pre')\n",
    "        predicted = model.predict_classes(token_list, verbose = 0)\n",
    "        \n",
    "        output_word = ''\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == predicted:\n",
    "                output_word = word\n",
    "                break\n",
    "        text += ' ' + output_word\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_line(\"gone were the\", 5, max_seq_len, model)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
