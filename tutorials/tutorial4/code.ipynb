{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# these are conditional probabilities, we may also want to have the prior probabilities of each state that\n",
    "# corresponds where it starts\n",
    "probabilities = [\n",
    "    [0.5, 0.3, 0.2],\n",
    "    [0.1, 0.2, 0.7],\n",
    "    [0.4, 0.4, 0.2]\n",
    "]\n",
    "\n",
    "# to get a probability of p(i|j)\n",
    "i = 1\n",
    "j = 2\n",
    "probabilities[i][j]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise experiment 1\n",
    "To get a deterministic sequence, the transition need to have a 100% probability in some cases. If we want some randomness, when we are in a state we can associate different probabilities to other states. If we want a more prominent state the probability of staying and going to that state should be higher when compared to others. If we don't want any repeated states in a sequence, the probability of going to the same state must be 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise experiment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1125\n",
      "0.06400000000000002\n",
      "0.010000000000000002\n",
      "0.18000000000000002\n",
      "0.051200000000000016\n"
     ]
    }
   ],
   "source": [
    "#defining the states\n",
    "THE = 0\n",
    "CAT = 1\n",
    "IS = 2\n",
    "FAT = 3\n",
    "\n",
    "prior = [0.9, 0.6, 0.8, 0.4]\n",
    "conditional_prob = [\n",
    "    [0, 0.5 , 0, 0.5],\n",
    "    [0.1, 0, 0.5, 0.4],\n",
    "    [0.4, 0.1, 0.0, 0.5],\n",
    "    [0.1, 0.8, 0.1, 0.0]\n",
    "]\n",
    "\n",
    "def get_prob(prior, conditional_prob, transitions):\n",
    "    assert len(transitions) > 0\n",
    "\n",
    "    last = transitions[0]\n",
    "    prob = prior[last]\n",
    "\n",
    "    for idx in range(1 , len(transitions)):\n",
    "        prob *= conditional_prob[transitions[idx-1]][transitions[idx]]\n",
    "    \n",
    "    return prob\n",
    "\n",
    "\n",
    "# THE CAT IS FAT\n",
    "STATES1 = [THE, CAT, IS, FAT]\n",
    "print(get_prob(prior, conditional_prob, STATES1))\n",
    "\n",
    "STATES2= [IS, THE, CAT, FAT]\n",
    "print(get_prob(prior, conditional_prob, STATES2))\n",
    "\n",
    "STATE5 = [FAT, THE, CAT, IS]\n",
    "print(get_prob(prior, conditional_prob, STATE5))\n",
    "\n",
    "STATES3= [THE, CAT, FAT]\n",
    "print(get_prob(prior, conditional_prob, STATES3))\n",
    "\n",
    "STATES4 = [IS, THE, FAT, CAT, FAT]\n",
    "print(get_prob(prior, conditional_prob, STATES4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since that for each state, there are some undeterministic behaviour (probabilities are not 1), the longer a sequence is, the lower the probability of getting that specific sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 1, 3, 2) 0.018000000000000002\n",
      "(0, 1, 2, 3) 0.1125\n",
      "(0, 3, 1, 2) 0.18000000000000002\n",
      "(0, 3, 2, 1) 0.0045000000000000005\n",
      "(1, 0, 3, 2) 0.003\n",
      "(1, 3, 2, 0) 0.009600000000000001\n",
      "(1, 2, 0, 3) 0.06\n",
      "(1, 2, 3, 0) 0.015\n",
      "(3, 0, 1, 2) 0.010000000000000002\n",
      "(3, 1, 2, 0) 0.06400000000000002\n",
      "(3, 2, 0, 1) 0.008000000000000002\n",
      "(3, 2, 1, 0) 0.00040000000000000013\n",
      "(2, 0, 1, 3) 0.06400000000000002\n",
      "(2, 0, 3, 1) 0.12800000000000003\n",
      "(2, 1, 0, 3) 0.004000000000000001\n",
      "(2, 1, 3, 0) 0.003200000000000001\n",
      "(2, 3, 0, 1) 0.020000000000000004\n",
      "(2, 3, 1, 0) 0.03200000000000001\n",
      "0.00040000000000000013\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "possible_states = list(itertools.permutations([THE, CAT, FAT, IS]))\n",
    "\n",
    "minimum = 1\n",
    "for transitions in possible_states:\n",
    "    prob = get_prob(prior, conditional_prob, transitions)\n",
    "    if prob == 0: # we only care about the possible ones\n",
    "        continue\n",
    "    minimum = min(minimum, prob)\n",
    "    print(transitions, prob)\n",
    "\n",
    "print(minimum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The least possible yet possible state is fat, is, cat, the\n",
    "\n",
    "THE = 0\n",
    "CAT = 1\n",
    "IS = 2\n",
    "FAT = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N-grams can be generated by using markov chains. When we are in a certain state(word), the next word we may generate depends on the probability to go to that new state(word). The probability depends on the training text we gave to the model and that way they build a statistical model. The N only represent the number of words generated, the first word depends on to the prior matrix and the next N-1 transitions depend on the conditional probabilities of the transition matrix.\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
